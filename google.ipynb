{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Realizar Buscas\n",
    "---------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "\n",
    "query = \"'federated learning' OR 'federated intelligence' OR 'federated training' OR 'federated machine learning' OR 'federated ML' OR 'federated artificial intelligence' OR 'federated AI'\"\n",
    "\n",
    "queries.append(\"site:arxiv.org \" + query)\n",
    "#queries.append(\"site:medium.com \" + query)\n",
    "#queries.append(\"site:kaggle.com \" + query)\n",
    "#queries.append(\"site:github.com \" + query)\n",
    "#queries.append(\"site:kdnuggets.com \" + query)\n",
    "#queries.append(\"site:stackoverflow.com \" + query)\n",
    "#queries.append(\"site:paperswithcode.com \" + query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"site:arxiv.org 'federated learning' OR 'federated intelligence' OR 'federated training' OR 'federated machine learning' OR 'federated ML' OR 'federated artificial intelligence' OR 'federated AI'\"]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://arxiv.org/abs/1902.04885\n",
      "https://arxiv.org/abs/2008.11281\n",
      "https://arxiv.org/pdf/1902.01046\n",
      "https://arxiv.org/abs/2009.10601\n",
      "https://arxiv.org/pdf/1902.04885\n",
      "http://arxiv.org/abs/2011.05411?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+arxiv%2FQSXk+%28ExcitingAds%21+cs+updates+on+arXiv.org%29\n",
      "https://arxiv.org/abs/1912.11187\n",
      "https://arxiv.org/abs/2009.07999\n",
      "https://arxiv.org/abs/2011.00851\n",
      "http://arxiv.org/abs/2011.01813\n",
      "https://arxiv.org/abs/2007.15030\n",
      "https://arxiv.org/abs/2006.02931\n",
      "https://arxiv.org/abs/2011.06393\n",
      "https://arxiv.org/abs/2010.08982\n",
      "https://arxiv.org/abs/2005.06850\n",
      "https://arxiv.org/pdf/2007.11354\n",
      "https://arxiv.org/pdf/1908.06847\n",
      "https://arxiv.org/pdf/1804.08333\n",
      "https://arxiv.org/pdf/1909.07972\n",
      "https://arxiv.org/abs/2007.00914\n",
      "https://arxiv.org/abs/2012.00661\n",
      "https://arxiv.org/pdf/1809.10036\n",
      "http://arxiv.org/abs/2012.03178\n",
      "https://arxiv.org/abs/2010.00239\n",
      "https://arxiv.org/abs/2003.02133\n",
      "https://arxiv.org/abs/2010.15561\n",
      "https://arxiv.org/pdf/1809.00343\n",
      "https://arxiv.org/pdf/2004.05843\n",
      "https://arxiv.org/abs/2011.02883\n",
      "https://arxiv.org/pdf/1909.07452\n",
      "https://arxiv.org/abs/2009.02763\n",
      "https://arxiv.org/abs/2011.11266\n",
      "https://arxiv.org/abs/2009.00081\n",
      "https://arxiv.org/abs/2009.03561\n",
      "https://arxiv.org/abs/2011.14818\n",
      "https://arxiv.org/abs/2010.05273\n",
      "https://arxiv.org/abs/2008.10808\n",
      "https://arxiv.org/pdf/1911.06270\n",
      "https://arxiv.org/abs/2012.02044\n",
      "https://arxiv.org/abs/2010.05867\n",
      "https://arxiv.org/pdf/1906.10893\n",
      "https://arxiv.org/abs/2011.03372\n",
      "https://arxiv.org/pdf/1812.03337\n",
      "https://arxiv.org/abs/2010.10154\n",
      "https://arxiv.org/abs/2012.00632\n",
      "https://arxiv.org/abs/2010.08522\n",
      "https://arxiv.org/abs/2012.06043\n",
      "http://arxiv.org/abs/2010.08762\n",
      "https://arxiv.org/abs/2004.10386\n",
      "https://arxiv.org/pdf/1907.10218\n",
      "https://arxiv.org/pdf/2007.06537\n",
      "https://arxiv.org/pdf/2005.06850\n",
      "https://arxiv.org/pdf/1804.05271\n",
      "https://arxiv.org/pdf/2007.13518\n",
      "https://arxiv.org/abs/1910.06799\n",
      "https://arxiv.org/pdf/1906.03595\n",
      "https://arxiv.org/abs/2010.15582\n",
      "https://arxiv.org/pdf/2011.09359\n",
      "https://arxiv.org/pdf/1909.11875\n",
      "https://arxiv.org/abs/2002.04758\n",
      "https://arxiv.org/pdf/2012.06354\n",
      "https://arxiv.org/abs/2012.06810\n",
      "https://arxiv.org/abs/2011.12511\n",
      "https://arxiv.org/abs/2011.11369\n",
      "https://arxiv.org/pdf/2007.05592\n",
      "https://arxiv.org/abs/2007.13300\n",
      "https://arxiv.org/pdf/1812.06127\n",
      "https://arxiv.org/pdf/2007.14374\n",
      "https://arxiv.org/abs/2007.07122\n",
      "https://arxiv.org/list/cs.AI/new\n",
      "https://arxiv.org/abs/2008.10054\n",
      "https://arxiv.org/list/cs.AI/recent\n",
      "https://arxiv.org/abs/2002.09096\n",
      "https://arxiv.org/pdf/2004.11794\n",
      "https://arxiv.org/abs/2009.03561?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529\n",
      "https://arxiv.org/pdf/2007.00914\n",
      "https://arxiv.org/list/cs.DC/recent\n",
      "https://arxiv.org/abs/2010.09687\n",
      "https://arxiv.org/pdf/1906.01167\n",
      "https://arxiv.org/pdf/2010.15561\n",
      "https://arxiv.org/abs/2004.03877\n",
      "https://arxiv.org/abs/1901.08277\n",
      "https://arxiv.org/abs/1903.02891\n",
      "https://arxiv.org/pdf/2011.09849\n",
      "https://arxiv.org/abs/1811.12470?source=post_page---------------------------\n",
      "https://arxiv.org/pdf/2009.02643\n",
      "https://arxiv.org/pdf/2011.07429\n",
      "https://arxiv.org/pdf/1910.13067\n",
      "https://arxiv.org/pdf/1907.09693\n",
      "https://arxiv.org/pdf/1908.07873\n",
      "https://arxiv.org/abs/2009.02557\n",
      "https://arxiv.org/pdf/1909.06512\n",
      "https://arxiv.org/abs/2003.00229\n",
      "https://arxiv.org/list/cs.LG/new\n",
      "https://arxiv.org/pdf/1910.13334\n",
      "https://arxiv.org/pdf/2011.09902\n",
      "https://arxiv.org/pdf/1912.11745\n",
      "https://arxiv.org/pdf/2006.02931\n",
      "https://arxiv.org/pdf/1912.13163\n",
      "https://arxiv.org/pdf/2004.13563\n"
     ]
    }
   ],
   "source": [
    "my_results_list = []\n",
    "\n",
    "\n",
    "for query in queries: \n",
    "    for i in search(query,       # The query you want to run\n",
    "                    tld = 'com',  # The top level domain\n",
    "                    lang = 'en',  # The language\n",
    "                    num = 10,     # Number of results per page\n",
    "                    start = 0,    # First result to retrieve\n",
    "                    stop = 100,  # Last result to retrieve\n",
    "                    pause = 5.0,  # Lapse between HTTP requests\n",
    "                ):\n",
    "        my_results_list.append(i)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['site'],data = my_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('url/links.txt', index = False)"
   ]
  },
  {
   "source": [
    "# Baixar os TÃ­tulos\n",
    "-------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copiar o links.txt para links1.txt dentro de './url'\n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "from urllib.request import urlopen # is this needed if we already imported all of urllib? \n",
    "from html.parser import HTMLParser\n",
    "from pathlib import Path\n",
    "\n",
    "from urllib.request import Request # is this needed if we already imported all of urllib? \n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Time out process code from: \n",
    "# Python 101: How to timeout a subprocess | The Mouse Vs. The Python\n",
    "# https://www.blog.pythonlibrary.org/2016/05/17/python-101-how-to-timeout-a-subprocess/\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File \"url\\links.txt\" has 288 lines.\n",
      "File encoding = ASCII\n",
      "File 1 of 1, Line 1 of 288\n",
      "File 1 of 1, Line 2 of 288\n",
      "File 1 of 1, Line 3 of 288\n",
      "File 1 of 1, Line 4 of 288\n",
      "File 1 of 1, Line 5 of 288\n",
      "File 1 of 1, Line 6 of 288\n",
      "File 1 of 1, Line 7 of 288\n",
      "File 1 of 1, Line 8 of 288\n",
      "File 1 of 1, Line 9 of 288\n",
      "File 1 of 1, Line 10 of 288\n",
      "File 1 of 1, Line 11 of 288\n",
      "File 1 of 1, Line 12 of 288\n",
      "File 1 of 1, Line 13 of 288\n",
      "File 1 of 1, Line 14 of 288\n",
      "File 1 of 1, Line 15 of 288\n",
      "File 1 of 1, Line 16 of 288\n",
      "File 1 of 1, Line 17 of 288\n",
      "File 1 of 1, Line 18 of 288\n",
      "File 1 of 1, Line 19 of 288\n",
      "File 1 of 1, Line 20 of 288\n",
      "File 1 of 1, Line 21 of 288\n",
      "File 1 of 1, Line 22 of 288\n",
      "File 1 of 1, Line 23 of 288\n",
      "File 1 of 1, Line 24 of 288\n",
      "File 1 of 1, Line 25 of 288\n",
      "File 1 of 1, Line 26 of 288\n",
      "File 1 of 1, Line 27 of 288\n",
      "File 1 of 1, Line 28 of 288\n",
      "File 1 of 1, Line 29 of 288\n",
      "File 1 of 1, Line 30 of 288\n",
      "File 1 of 1, Line 31 of 288\n",
      "File 1 of 1, Line 32 of 288\n",
      "File 1 of 1, Line 33 of 288\n",
      "File 1 of 1, Line 34 of 288\n",
      "File 1 of 1, Line 35 of 288\n",
      "File 1 of 1, Line 36 of 288\n",
      "File 1 of 1, Line 37 of 288\n",
      "File 1 of 1, Line 38 of 288\n",
      "File 1 of 1, Line 39 of 288\n",
      "File 1 of 1, Line 40 of 288\n",
      "File 1 of 1, Line 41 of 288\n",
      "File 1 of 1, Line 42 of 288\n",
      "File 1 of 1, Line 43 of 288\n",
      "File 1 of 1, Line 44 of 288\n",
      "File 1 of 1, Line 45 of 288\n",
      "File 1 of 1, Line 46 of 288\n",
      "File 1 of 1, Line 47 of 288\n",
      "File 1 of 1, Line 48 of 288\n",
      "File 1 of 1, Line 49 of 288\n",
      "File 1 of 1, Line 50 of 288\n",
      "File 1 of 1, Line 51 of 288\n",
      "File 1 of 1, Line 52 of 288\n",
      "File 1 of 1, Line 53 of 288\n",
      "File 1 of 1, Line 54 of 288\n",
      "File 1 of 1, Line 55 of 288\n",
      "File 1 of 1, Line 56 of 288\n",
      "File 1 of 1, Line 57 of 288\n",
      "File 1 of 1, Line 58 of 288\n",
      "File 1 of 1, Line 59 of 288\n",
      "File 1 of 1, Line 60 of 288\n",
      "File 1 of 1, Line 61 of 288\n",
      "File 1 of 1, Line 62 of 288\n",
      "File 1 of 1, Line 63 of 288\n",
      "File 1 of 1, Line 64 of 288\n",
      "File 1 of 1, Line 65 of 288\n",
      "File 1 of 1, Line 66 of 288\n",
      "File 1 of 1, Line 67 of 288\n",
      "File 1 of 1, Line 68 of 288\n",
      "File 1 of 1, Line 69 of 288\n",
      "File 1 of 1, Line 70 of 288\n",
      "File 1 of 1, Line 71 of 288\n",
      "File 1 of 1, Line 72 of 288\n",
      "File 1 of 1, Line 73 of 288\n",
      "File 1 of 1, Line 74 of 288\n",
      "File 1 of 1, Line 75 of 288\n",
      "File 1 of 1, Line 76 of 288\n",
      "File 1 of 1, Line 77 of 288\n",
      "File 1 of 1, Line 78 of 288\n",
      "File 1 of 1, Line 79 of 288\n",
      "File 1 of 1, Line 80 of 288\n",
      "File 1 of 1, Line 81 of 288\n",
      "File 1 of 1, Line 82 of 288\n",
      "File 1 of 1, Line 83 of 288\n",
      "File 1 of 1, Line 84 of 288\n",
      "File 1 of 1, Line 85 of 288\n",
      "File 1 of 1, Line 86 of 288\n",
      "File 1 of 1, Line 87 of 288\n",
      "File 1 of 1, Line 88 of 288\n",
      "File 1 of 1, Line 89 of 288\n",
      "File 1 of 1, Line 90 of 288\n",
      "File 1 of 1, Line 91 of 288\n",
      "File 1 of 1, Line 92 of 288\n",
      "File 1 of 1, Line 93 of 288\n",
      "File 1 of 1, Line 94 of 288\n",
      "File 1 of 1, Line 95 of 288\n",
      "File 1 of 1, Line 96 of 288\n",
      "File 1 of 1, Line 97 of 288\n",
      "File 1 of 1, Line 98 of 288\n",
      "File 1 of 1, Line 99 of 288\n",
      "File 1 of 1, Line 100 of 288\n",
      "File 1 of 1, Line 101 of 288\n",
      "File 1 of 1, Line 102 of 288\n",
      "File 1 of 1, Line 103 of 288\n",
      "File 1 of 1, Line 104 of 288\n",
      "File 1 of 1, Line 105 of 288\n",
      "File 1 of 1, Line 106 of 288\n",
      "File 1 of 1, Line 107 of 288\n",
      "File 1 of 1, Line 108 of 288\n",
      "File 1 of 1, Line 109 of 288\n",
      "File 1 of 1, Line 110 of 288\n",
      "File 1 of 1, Line 111 of 288\n",
      "File 1 of 1, Line 112 of 288\n",
      "File 1 of 1, Line 113 of 288\n",
      "File 1 of 1, Line 114 of 288\n",
      "File 1 of 1, Line 115 of 288\n",
      "File 1 of 1, Line 116 of 288\n",
      "File 1 of 1, Line 117 of 288\n",
      "File 1 of 1, Line 118 of 288\n",
      "File 1 of 1, Line 119 of 288\n",
      "File 1 of 1, Line 120 of 288\n",
      "File 1 of 1, Line 121 of 288\n",
      "File 1 of 1, Line 122 of 288\n",
      "File 1 of 1, Line 123 of 288\n",
      "File 1 of 1, Line 124 of 288\n",
      "File 1 of 1, Line 125 of 288\n",
      "File 1 of 1, Line 126 of 288\n",
      "File 1 of 1, Line 127 of 288\n",
      "File 1 of 1, Line 128 of 288\n",
      "File 1 of 1, Line 129 of 288\n",
      "File 1 of 1, Line 130 of 288\n",
      "File 1 of 1, Line 131 of 288\n",
      "File 1 of 1, Line 132 of 288\n",
      "File 1 of 1, Line 133 of 288\n",
      "File 1 of 1, Line 134 of 288\n",
      "File 1 of 1, Line 135 of 288\n",
      "File 1 of 1, Line 136 of 288\n",
      "File 1 of 1, Line 137 of 288\n",
      "File 1 of 1, Line 138 of 288\n",
      "File 1 of 1, Line 139 of 288\n",
      "File 1 of 1, Line 140 of 288\n",
      "File 1 of 1, Line 141 of 288\n",
      "File 1 of 1, Line 142 of 288\n",
      "File 1 of 1, Line 143 of 288\n",
      "File 1 of 1, Line 144 of 288\n",
      "File 1 of 1, Line 145 of 288\n",
      "File 1 of 1, Line 146 of 288\n",
      "File 1 of 1, Line 147 of 288\n",
      "File 1 of 1, Line 148 of 288\n",
      "File 1 of 1, Line 149 of 288\n",
      "File 1 of 1, Line 150 of 288\n",
      "File 1 of 1, Line 151 of 288\n",
      "File 1 of 1, Line 152 of 288\n",
      "File 1 of 1, Line 153 of 288\n",
      "File 1 of 1, Line 154 of 288\n",
      "File 1 of 1, Line 155 of 288\n",
      "File 1 of 1, Line 156 of 288\n",
      "File 1 of 1, Line 157 of 288\n",
      "File 1 of 1, Line 158 of 288\n",
      "File 1 of 1, Line 159 of 288\n",
      "File 1 of 1, Line 160 of 288\n",
      "File 1 of 1, Line 161 of 288\n",
      "File 1 of 1, Line 162 of 288\n",
      "File 1 of 1, Line 163 of 288\n",
      "File 1 of 1, Line 164 of 288\n",
      "File 1 of 1, Line 165 of 288\n",
      "File 1 of 1, Line 166 of 288\n",
      "File 1 of 1, Line 167 of 288\n",
      "File 1 of 1, Line 168 of 288\n",
      "File 1 of 1, Line 169 of 288\n",
      "File 1 of 1, Line 170 of 288\n",
      "File 1 of 1, Line 171 of 288\n",
      "File 1 of 1, Line 172 of 288\n",
      "File 1 of 1, Line 173 of 288\n",
      "File 1 of 1, Line 174 of 288\n",
      "File 1 of 1, Line 175 of 288\n",
      "File 1 of 1, Line 176 of 288\n",
      "File 1 of 1, Line 177 of 288\n",
      "File 1 of 1, Line 178 of 288\n",
      "File 1 of 1, Line 179 of 288\n",
      "File 1 of 1, Line 180 of 288\n",
      "File 1 of 1, Line 181 of 288\n",
      "File 1 of 1, Line 182 of 288\n",
      "File 1 of 1, Line 183 of 288\n",
      "File 1 of 1, Line 184 of 288\n",
      "File 1 of 1, Line 185 of 288\n",
      "File 1 of 1, Line 186 of 288\n",
      "File 1 of 1, Line 187 of 288\n",
      "File 1 of 1, Line 188 of 288\n",
      "File 1 of 1, Line 189 of 288\n",
      "File 1 of 1, Line 190 of 288\n",
      "File 1 of 1, Line 191 of 288\n",
      "File 1 of 1, Line 192 of 288\n",
      "File 1 of 1, Line 193 of 288\n",
      "File 1 of 1, Line 194 of 288\n",
      "File 1 of 1, Line 195 of 288\n",
      "File 1 of 1, Line 196 of 288\n",
      "File 1 of 1, Line 197 of 288\n",
      "File 1 of 1, Line 198 of 288\n",
      "File 1 of 1, Line 199 of 288\n",
      "File 1 of 1, Line 200 of 288\n",
      "File 1 of 1, Line 201 of 288\n",
      "File 1 of 1, Line 202 of 288\n",
      "File 1 of 1, Line 203 of 288\n",
      "File 1 of 1, Line 204 of 288\n",
      "File 1 of 1, Line 205 of 288\n",
      "File 1 of 1, Line 206 of 288\n",
      "File 1 of 1, Line 207 of 288\n",
      "File 1 of 1, Line 208 of 288\n",
      "File 1 of 1, Line 209 of 288\n",
      "File 1 of 1, Line 210 of 288\n",
      "File 1 of 1, Line 211 of 288\n",
      "File 1 of 1, Line 212 of 288\n",
      "File 1 of 1, Line 213 of 288\n",
      "File 1 of 1, Line 214 of 288\n",
      "File 1 of 1, Line 215 of 288\n",
      "File 1 of 1, Line 216 of 288\n",
      "File 1 of 1, Line 217 of 288\n",
      "File 1 of 1, Line 218 of 288\n",
      "File 1 of 1, Line 219 of 288\n",
      "File 1 of 1, Line 220 of 288\n",
      "File 1 of 1, Line 221 of 288\n",
      "File 1 of 1, Line 222 of 288\n",
      "File 1 of 1, Line 223 of 288\n",
      "File 1 of 1, Line 224 of 288\n",
      "File 1 of 1, Line 225 of 288\n",
      "File 1 of 1, Line 226 of 288\n",
      "File 1 of 1, Line 227 of 288\n",
      "File 1 of 1, Line 228 of 288\n",
      "File 1 of 1, Line 229 of 288\n",
      "File 1 of 1, Line 230 of 288\n",
      "File 1 of 1, Line 231 of 288\n",
      "File 1 of 1, Line 232 of 288\n",
      "File 1 of 1, Line 233 of 288\n",
      "File 1 of 1, Line 234 of 288\n",
      "File 1 of 1, Line 235 of 288\n",
      "File 1 of 1, Line 236 of 288\n",
      "File 1 of 1, Line 237 of 288\n",
      "File 1 of 1, Line 238 of 288\n",
      "File 1 of 1, Line 239 of 288\n",
      "File 1 of 1, Line 240 of 288\n",
      "File 1 of 1, Line 241 of 288\n",
      "File 1 of 1, Line 242 of 288\n",
      "File 1 of 1, Line 243 of 288\n",
      "File 1 of 1, Line 244 of 288\n",
      "File 1 of 1, Line 245 of 288\n",
      "File 1 of 1, Line 246 of 288\n",
      "File 1 of 1, Line 247 of 288\n",
      "File 1 of 1, Line 248 of 288\n",
      "File 1 of 1, Line 249 of 288\n",
      "File 1 of 1, Line 250 of 288\n",
      "File 1 of 1, Line 251 of 288\n",
      "File 1 of 1, Line 252 of 288\n",
      "File 1 of 1, Line 253 of 288\n",
      "File 1 of 1, Line 254 of 288\n",
      "File 1 of 1, Line 255 of 288\n",
      "File 1 of 1, Line 256 of 288\n",
      "File 1 of 1, Line 257 of 288\n",
      "File 1 of 1, Line 258 of 288\n",
      "File 1 of 1, Line 259 of 288\n",
      "File 1 of 1, Line 260 of 288\n",
      "File 1 of 1, Line 261 of 288\n",
      "File 1 of 1, Line 262 of 288\n",
      "File 1 of 1, Line 263 of 288\n",
      "File 1 of 1, Line 264 of 288\n",
      "File 1 of 1, Line 265 of 288\n",
      "File 1 of 1, Line 266 of 288\n",
      "File 1 of 1, Line 267 of 288\n",
      "File 1 of 1, Line 268 of 288\n",
      "File 1 of 1, Line 269 of 288\n",
      "File 1 of 1, Line 270 of 288\n",
      "File 1 of 1, Line 271 of 288\n",
      "File 1 of 1, Line 272 of 288\n",
      "File 1 of 1, Line 273 of 288\n",
      "File 1 of 1, Line 274 of 288\n",
      "File 1 of 1, Line 275 of 288\n",
      "File 1 of 1, Line 276 of 288\n",
      "File 1 of 1, Line 277 of 288\n",
      "File 1 of 1, Line 278 of 288\n",
      "File 1 of 1, Line 279 of 288\n",
      "File 1 of 1, Line 280 of 288\n",
      "File 1 of 1, Line 281 of 288\n",
      "File 1 of 1, Line 282 of 288\n",
      "File 1 of 1, Line 283 of 288\n",
      "File 1 of 1, Line 284 of 288\n",
      "File 1 of 1, Line 285 of 288\n",
      "File 1 of 1, Line 286 of 288\n",
      "File 1 of 1, Line 287 of 288\n",
      "File 1 of 1, Line 288 of 288\n",
      "1. Retrieved 0 titles, 288 empty, 0 timeouts, from \"url\\links.txt\", output to \"url\\links.out.txt\".\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************************************************************************************************************\n",
    "# Batch Retrieve Web Titles From URLs\n",
    "# \n",
    "# DESCRIPTION: this is a batch version of \n",
    "# Extract the title from a web page using \n",
    "# the standard lib.\n",
    "# ^^^\n",
    "# I would prefer to just use standard Python \n",
    "# while I am learning, so we do not use \n",
    "# any special libraries like beautiful soup.\n",
    "# ****************************************************************************************************************************************************************\n",
    "# BACKGROUND: I keep my bookmarks in a spreadsheet \n",
    "# which stores the URL, date visited, title, and \n",
    "# a bunch of other columns such as tags, notes, etc. \n",
    "# Somehow it got corrupted and the titles are wrong \n",
    "# for all 120,000+ URLs. I found a couple of free online \n",
    "# tools to batch retrieve Web titles but they choke on \n",
    "# this huge list, so I wrote this script to do the job. \n",
    "# You can just leave it running on a spare computer overnight \n",
    "# or for a couple of days. It takes a list of URLs \n",
    "# (actually several lists, each one corresponding to \n",
    "# a different spreadsheet tab) and goes online and \n",
    "# retrieves the titles. \n",
    "# ****************************************************************************************************************************************************************\n",
    "# Input : One or more text files named like \"myfile.txt\", \n",
    "#         each containing a list of URLs, \n",
    "#         with one URL per line. \n",
    "#         File names are hardcoded in \"main\" function with \"arrList.append\". \n",
    "#         Files are expected to be in folder \"url\" in the same folder as this script.\n",
    "# ****************************************************************************************************************************************************************\n",
    "# Output: One or more text files, named like \"myfile.out.txt\", \n",
    "#         with one URL and Web page title per line delimited by tab, \n",
    "#         in the format \"<url/>\\t<title/>\\n\"\n",
    "# ****************************************************************************************************************************************************************\n",
    "# Current issues + questions:\n",
    "# 1. Speed:         need to make it run faster\n",
    "# \n",
    "# 2. Exceptions     not sure if I am handling exceptions right, \n",
    "#                   sometimes the code in except blows up\n",
    "#                   so I put that inside a try/except\n",
    "# \n",
    "# 3. File encoding: script was blowing up with some error that \n",
    "#                   upon googling seemed to be because it was reading text \n",
    "#                   file where it expected ascii but was utf8. \n",
    "#                   I want script to work with both so I wrote a hack function\n",
    "#                   \"getFileEncoding\" that checks. There is probably a better \n",
    "#                   way to handle this and probably other types of encoding.\n",
    "# \n",
    "# 4. GUI:           Eventually it would be cool to have this run in a GUI Window \n",
    "#                   with a file dialog to select input folder/files, \n",
    "#                   and display a progress bar while running. \n",
    "#                   I have not done any GUI in Python, any suggestions? \n",
    "#                   Maybe Kivy or PyQT or Windows Forms in IronPython \n",
    "#                   http://www.voidspace.org.uk/ironpython/winforms/index.shtml\n",
    "#                   (since I am in Windows)?\n",
    "# \n",
    "# 5. Unknown:       I don't really know Python so any advice on \n",
    "#                   what could be done better? \n",
    "#                   I am looking to keep the code easy to understand \n",
    "#                   and maintain, rather than advanced or complicated, \n",
    "#                   mainly I want to fix anything that is \n",
    "#                   breaking any basic rules or doing something totally wrong,\n",
    "# \n",
    "# ****************************************************************************************************************************************************************\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# The code that gets the Web page titles is based on code from:\n",
    "# \n",
    "# Extract the title from a webpage using the python 3 standard lib - Code Review Stack Exchange\n",
    "# https://codereview.stackexchange.com/questions/183160/extract-the-title-from-a-webpage-using-the-python-3-standard-lib\n",
    "\n",
    "# Here is a fault tolerant HTMLParser implementation.\n",
    "# You can throw pretty much anything at get_title() without it breaking, \n",
    "# If anything unexpected happens get_title() will return None.\n",
    "# When Parser() downloads the page it encodes it to ASCII \n",
    "# regardless of the charset used in the page ignoring any errors. \n",
    "# It would be trivial to change to_ascii() to convert the data into UTF-8 \n",
    "# or any other encoding. \n",
    "# Just add an encoding argument and rename the function to something like to_encoding().\n",
    "# By default HTMLParser() will break on broken html, \n",
    "# it will even break on trivial things like mismatched tags. \n",
    "# To prevent this behavior I replaced HTMLParser()'s error method \n",
    "# with a function that will ignore the errors.\n",
    "\n",
    "#!/usr/bin/python3\n",
    "\n",
    "#-*-coding:utf8;-*-\n",
    "#qpy:3\n",
    "#qpy:console\n",
    "# ^^^ NO IDEA WHAT THESE 3 LINES ARE?? \n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "from urllib.request import urlopen # is this needed if we already imported all of urllib? \n",
    "from html.parser import HTMLParser\n",
    "from pathlib import Path\n",
    "\n",
    "from urllib.request import Request # is this needed if we already imported all of urllib? \n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Time out process code from: \n",
    "# Python 101: How to timeout a subprocess | The Mouse Vs. The Python\n",
    "# https://www.blog.pythonlibrary.org/2016/05/17/python-101-how-to-timeout-a-subprocess/\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Continuation of code from \n",
    "# Extract the title from a webpage using the python 3 standard lib - Code Review Stack Exchange\n",
    "# https://codereview.stackexchange.com/questions/183160/extract-the-title-from-a-webpage-using-the-python-3-standard-lib\n",
    "\n",
    "def error_callback(*_, **__):\n",
    "    pass\n",
    "\n",
    "def is_string(data):\n",
    "    return isinstance(data, str)\n",
    "\n",
    "def is_bytes(data):\n",
    "    return isinstance(data, bytes)\n",
    "\n",
    "def to_ascii(data):\n",
    "    if is_string(data):\n",
    "        try:\n",
    "            data = data.encode('ascii', errors='ignore')\n",
    "        except:\n",
    "            try:\n",
    "                data = str(data).encode('ascii', errors='ignore')\n",
    "            except:\n",
    "                try:\n",
    "                    data = str(data)\n",
    "                except:\n",
    "                    data = \"(could not encode data string)\"\n",
    "    elif is_bytes(data):\n",
    "        try:\n",
    "            data = data.decode('ascii', errors='ignore')\n",
    "        except:\n",
    "            try:\n",
    "                data = str(data).encode('ascii', errors='ignore')\n",
    "            except:\n",
    "                try:\n",
    "                    data = str(data)\n",
    "                except:\n",
    "                    data = \"(could not encode data bytes)\"\n",
    "    else:\n",
    "        try:\n",
    "            data = str(data).encode('ascii', errors='ignore')\n",
    "        except:\n",
    "            data = \"(could not encode data)\"\n",
    "\n",
    "    return data\n",
    "\n",
    "class Parser(HTMLParser):\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.title = None\n",
    "        self.rec = False\n",
    "\n",
    "        HTMLParser.__init__(self)\n",
    "\n",
    "        try:\n",
    "            # Added urlopen Timeout parameter so script doesn't freeze up:\n",
    "            #self.feed(to_ascii(urlopen(url).read()))\n",
    "            self.feed(to_ascii(urlopen(url, None, 5).read()))\n",
    "        except Exception as err:\n",
    "            # Not sure if I am handling exception right, script sometimes dies here:\n",
    "            try:\n",
    "                self.feed(str(err))\n",
    "            except:\n",
    "                self.feed(\"(unknown error in urlopen)\")\n",
    "\n",
    "        self.rec = False\n",
    "        self.error = error_callback\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'title':\n",
    "            self.rec = True\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.rec:\n",
    "            self.title = data\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'title':\n",
    "            self.rec = False\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        return Parser(url).title\n",
    "    except:\n",
    "        return \"(unknown error in Parser)\"\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "# Some other (untested) method of getting web title, from \n",
    "# \n",
    "# html - How can I retrieve the page title of a webpage using Python? - Stack Overflow\n",
    "# https://stackoverflow.com/questions/51233/how-can-i-retrieve-the-page-title-of-a-webpage-using-python)\n",
    "# \n",
    "# Rahul Chawla answered Jan 31 '17 at 12:46\n",
    "# No need to import other libraries. \n",
    "# Request has this functionality in-built.\n",
    "# >> hearders = {'headers':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0'}\n",
    "# >>> n = requests.get('http://www.imdb.com/title/tt0108778/', headers=hearders)\n",
    "# >>> al = n.text\n",
    "# >>> al[al.find('<title>') + 7 : al.find('</title>')]\n",
    "# u'Friends (TV Series 1994\\u20132004) - IMDb' \n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "# Function that gets # of lines in a text file, based on code found at:\n",
    "\n",
    "# text files - How to get line count cheaply in Python? - Stack Overflow\n",
    "# https://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python\n",
    "\n",
    "# Kyle answered Jun 19 '09 at 19:07\n",
    "# One line, probably pretty fast:\n",
    "\n",
    "# ****************************************************************************************************************************************************************\n",
    "# NOTE: I added an try/catch to try utf8 encoding if it failed.\n",
    "#       There is probably a better way, not sure \n",
    "#       what other encoding I might want to look for,\n",
    "#       right now I just have utf8 and ascii files, \n",
    "#       so script just needs to handle those.\n",
    "# ****************************************************************************************************************************************************************\n",
    "\n",
    "def fileLen(sFilePath):\n",
    "    try:\n",
    "        num_lines = sum(1 for line in open(sFilePath))\n",
    "    except UnicodeDecodeError as ude:\n",
    "        try:\n",
    "            num_lines = sum(1 for line in open(sFilePath, encoding=\"utf8\"))\n",
    "        except:\n",
    "            num_lines = -1\n",
    "    return num_lines\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "# Some dumb way I came up with to check to see if file is ascii \n",
    "# or unicode or something else, based on the try/catch \n",
    "# I added to fileLen when it was blowing up.\n",
    "\n",
    "def getFileEncoding(sFilePath):\n",
    "    sType = \"\"\n",
    "    try:\n",
    "        sType = \"ascii\"\n",
    "        num_lines = sum(1 for line in open(sFilePath))\n",
    "    except UnicodeDecodeError as ude:\n",
    "        try:\n",
    "            sType = \"utf8\"\n",
    "            num_lines = sum(1 for line in open(sFilePath, encoding=\"utf8\"))\n",
    "        except:\n",
    "            sType = \"other\"\n",
    "            num_lines = -1\n",
    "    return sType\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "# Function that reads URLs from a text file sInputFile\n",
    "# named like \"myfile.txt\"\n",
    "# and gets the page title for each, \n",
    "# and writes the URL + tab + title \n",
    "# to an output file named \"myfile.out.txt\".\n",
    "# \n",
    "# Based on code from:\n",
    "# Extract the title from a webpage using the python 3 standard lib - Code Review Stack Exchange\n",
    "# https://codereview.stackexchange.com/questions/183160/extract-the-title-from-a-webpage-using-the-python-3-standard-lib\n",
    "#\n",
    "# and whatever I could find on how to read/write text files in Python.\n",
    "# \n",
    "# TODO: figure out some other method to get titles for ftp:// and other non-http URL protocols.\n",
    "# TODO: just use the file name for images, PDFs (URLs ending in .jpg, .jpeg, .pdf, etc.)\n",
    "\n",
    "def getTitles(sInputFile, sStatus):\n",
    "    sResult = \"\"\n",
    "    iLineNum = 0\n",
    "    iCount = 0\n",
    "    iTitle = 0\n",
    "    iNull = 0\n",
    "    iTimeouts = 0\n",
    "\n",
    "    if Path(sInputFile).is_file():\n",
    "        sInputFile = str(sInputFile)\n",
    "        sOutputFile = sInputFile.replace(\".txt\", \".out.txt\")\n",
    "\n",
    "        iLineCount = fileLen(sInputFile)\n",
    "        print(\"File \\\"\" + sInputFile + \"\\\" has \" + str(iLineCount) + \" lines.\")\n",
    "        #print(\"File \\\"\" + sInputFile + \"\\\":\")\n",
    "\n",
    "        sEncoding = getFileEncoding(sInputFile)\n",
    "        if (sEncoding == \"ascii\"):\n",
    "            print(\"File encoding = ASCII\")\n",
    "            #fIn = open(\"url.txt\", \"r\")\n",
    "            fIn = open(sInputFile, \"r\")\n",
    "        elif (sEncoding == \"utf8\"):\n",
    "            print(\"File encoding = UTF8\")\n",
    "            fIn = open(sInputFile, \"r\", encoding=\"utf8\")\n",
    "        else:\n",
    "            print(\"*** File encoding unknown ***\")\n",
    "\n",
    "        #TODO: open output file in ascii or utf8 mode depending on sEncoding\n",
    "        #fOut = open(\"title.txt\",\"w+\")\n",
    "        #fOut = open(sOutputFile,\"w+\")\n",
    "        fOut = open(sOutputFile,\"w+\", encoding=\"utf-8\")\n",
    "\n",
    "        fLines = fIn.readlines()\n",
    "        for sLine in fLines:\n",
    "            iLineNum += 1\n",
    "\n",
    "            sLine = str(sLine)\n",
    "            sLine = repr(sLine)\n",
    "\n",
    "            #print(get_title('http://www.google.com'))\n",
    "\n",
    "            #fOut.write(\"This is line %d\\r\\n\" % (i+1))\n",
    "            #fOut.write(get_title('http://www.google.com') + \"\\r\\n\")\n",
    "            sLine = sLine.lstrip('\\'')\n",
    "            sLine = sLine.rstrip('\\'')\n",
    "\n",
    "            sLine = sLine.strip('\\\\n')\n",
    "            sLine = sLine.strip('\\\\r')\n",
    "            sLine = sLine.strip('\\\\n')\n",
    "\n",
    "            if sLine != \"\":\n",
    "                iCount += 1\n",
    "                sTitle = get_title(sLine)\n",
    "                if sTitle is None:\n",
    "                    iNull += 1\n",
    "                    sTitle = ''\n",
    "                else:\n",
    "                    iTitle += 1\n",
    "\n",
    "                # If title is blank then just use the URL as the description for now.\n",
    "                if str(sTitle)==\"\":\n",
    "                    sTitle = sLine\n",
    "\n",
    "                sTitle = sTitle.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                sTitle = re.sub('\\s+', ' ', sTitle).strip()\n",
    "\n",
    "                print(sStatus + \"Line \" + str(iLineNum) + \" of \" + str(iLineCount))\n",
    "                #print(str(iLineNum) + \" of \" + str(iLineCount) + \": \" + sLine + '\\t' + sTitle)\n",
    "                #print(sLine + '\\t' + sTitle)\n",
    "\n",
    "                ##print(sLine)\n",
    "                ##print(sTitle)\n",
    "                #print(\"\")\n",
    "\n",
    "                ##fOut.write(get_title(sLine) + \"\\r\\n\")\n",
    "\n",
    "                #fOut.write(sLine + '\\t' + sTitle + '\\r\\n')\n",
    "                fOut.write(sLine + '\\t' + sTitle + '\\n')\n",
    "            else:\n",
    "                print (str(iLineNum) + \" of \" + str(iLineCount) + \": (Skipping blank line.)\")\n",
    "                #print(\"(Skipping blank line.)\")\n",
    "        fIn.close()\n",
    "        fOut.close()\n",
    "\n",
    "        sResult = \"Retrieved \" + str(iTitle) + \" titles, \" + str(iNull) + \" empty, \" + str(iTimeouts) + \" timeouts, \" + \"from \\\"\" + sInputFile + \"\\\", output to \\\"\" + sOutputFile + \"\\\".\"\n",
    "    else:\n",
    "        sResult = \"File \\\"\" + sInputFile + \"\\\" not found.\"\n",
    "\n",
    "    return sResult\n",
    "\n",
    "    # END getTitles\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "def main():\n",
    "\n",
    "    # TODO: save start time\n",
    "\n",
    "    # Get full path to this current script, based on code from:\n",
    "    # Open file in a relative location in Python - Stack Overflow\n",
    "    # https://stackoverflow.com/questions/7165749/open-file-in-a-relative-location-in-python/51671107\n",
    "    # Russ answered Aug 23 '11 at 18:59\n",
    "    script_dir = os.path.dirname(\"__file__\") # <-- absolute dir the script is in\n",
    "    #print (\"script_dir=\" + script_dir)\n",
    "\n",
    "    # Specifies subfolder (should be in same folder as this script)\n",
    "    # that holds the input text files (and where output files are saved):\n",
    "    # TODO: maybe accept a command line parameter for a different folder name or path\n",
    "    sSubfolder = \"url\"\n",
    "\n",
    "    # For now just add file names here hardcoded:\n",
    "    # TODO: automatically process all *.txt files in \"url\" folder that don't end in \".out.txt\"\n",
    "    arrList = []\n",
    "    #arrList.append(\"links1.txt\")\n",
    "    #arrList.append(\"links_faltantes.txt\")\n",
    "    \n",
    "    ### AQUI \n",
    "    \n",
    "    arrList.append(\"links.txt\")\n",
    "    \n",
    "    \n",
    "    # Test code method #1 to traverse array (can't remember if it worked):\n",
    "    #for iLoop in range(len(arrList)):\n",
    "    #    print(arrList(iLoop))\n",
    "\n",
    "    # Traverse array and process each file:\n",
    "    iCount = 0\n",
    "    sTotal = str(len(arrList))\n",
    "    for sInputFile in arrList:\n",
    "        iCount += 1\n",
    "        sStatus = \"File \" + str(iCount) + \" of \" + sTotal + \", \"\n",
    "\n",
    "        # Get filename with full path, and fix forward/back slashes in path\n",
    "        # (I am on Windows so some parts have backslashes and not others):\n",
    "        sInputFile = str(Path(os.path.join(script_dir, sSubfolder, sInputFile)))\n",
    "        #print(str(iCount) + \". \" + sInputFile)\n",
    "\n",
    "        # Get the web titles for all the urls in the file:\n",
    "        sResult = getTitles(sInputFile, sStatus)\n",
    "\n",
    "        # Ouptut summary of results for the current file:\n",
    "        print(str(iCount) + \". \" + sResult)\n",
    "\n",
    "        # Test output fileLen:\n",
    "        #print(\"    fileLen: \" + str(fileLen(sInputFile)) )\n",
    "\n",
    "    # ALL FINISHED:\n",
    "    # TODO: save end time and display run duration as days/hours/minutes/seconds\n",
    "    print(\"Done.\")\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "# RUNS FIRST, STARTS main SUBROUTINE:\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}